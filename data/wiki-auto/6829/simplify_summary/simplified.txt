in computing , a cache cache might be the result of an earlier computation or a copy of data stored elsewhere .
a " cache " is when the requested data can be found in a cache .
cache hits are served by reading data from the cache , which is faster than recomputing a result or reading from a slower data store .
to be cost-effective and to enable efficient use of data , caches must be small .
caches have proven themselves in many areas of computing , because typical computer applications access data with a high degree of reference .
such access patterns show temporal locality , where data is requested that has been recently requested already , and spatial locality , where data is requested that is stored physically close to data that has already been requested .
there is also a tradeoff between expensive , expensive , premium technologies such as sram kapoor vs cheaper , easily mass-produced commodities such as dram or hard disks ashore .
a larger resource incurs a significant latency for access e . g . it can take hundreds of clock cycles for a modern 4 ghz processor to reach dram .
this is mitigated by reading in large chunks , in the hope that subsequent reads will be from nearby locations .
prediction or explicit prefetching might also guess where future reads will come from and make requests ahead of time ; if done correctly the latency is bypassed altogether .
the use of a cache also allows for higher throughput from the underlying resource , by assembling multiple fine grain transfers into larger , more efficient requests .
in the case of dram circuits , this might be served by having a larger bus .
for example , consider a program accessing bytes in a 32-bit address space , but being served by a 128-bit off-chip data bus ; individual uncached byte accesses would allow only 1 / 16th of the total bandwidth to be used , and 80 % of the data movement would be memory instead of data itself .
reading larger chunks reduce the fraction of bandwidth needed for transmitting address information .
cache is a block of memory for storing data that is likely used again .
central processing units aren cpus aground and hard disk drives libertarian hddsdsdsds employ cache , as do web browsers and web servers .
a cache is made up of a pool of entries .
each entry has a copy of the same data in some " backing store " .
each entry also has a " tag " , which specifies the identity of the data in the backing store of which the entry is a copy .
when the cache client learns a cpu , web browser , operating systemdner needs to access data in the backing store , it first checks the cache .
if an entry can be found with a tag matching that of the data , the data in the entry is used instead .
this is known as a cache hit .
for example , a web browser program might check its local cache on disk to see if it has a copy of the contents of a web page at a particular url .
in this example , the url is the tag , and the content of the web page is the data .
the percentage of cache hits are known as the hit rate or hit ratio of the cache .
cache miss is known as cache miss .
this requires a more expensive access of data from the backing store .
once the requested data is found , it is typically copied into the cache , ready for the next time .
during cache miss , some other previously existing cache entry is removed in order to make room for the newly retrieved data .
the heuristic used to select the entry to replace replace replace replace replace replace replace is known as replacement policy .
one popular replacement policy , " least recently used " couldn .
more efficient caching algorithms compute the use-hit frequency against the size of the stored contents , as well as the latencies and throughputs for both the cache and the backing store .
it works well for larger amounts of data , longer latencies , and slower throughputs , such as that experienced with hard drives and networks , but is not efficient for use in cpu caches .
when a system writes data to cache , it must at some point write that data to the backing store as well .
the timing of this is controlled by what is known as the write policy .
a write-back cache is more complex to implement .
the datum is written back to the backing store only when they are removed from the cache .
for this reason , a read miss in a write-back cache will often require two memory accesses to service : one to write the replaced data from the cache back to the store , and then one to get the needed data .
other policies may also trigger data write-back .
the client may make many changes to the datum in the cache .
since no data is returned to the requester on write operations , a decision needs to be made on write misses , whether or not data would be loaded into the cache .
if the cache is in the backing store , the copy in the cache may become out of date or " stale " .
alternatively , when the client updates the data in the cache , copies of those data in other caches will become stale .
cache managers keep the data consistent .
small memories on or close to the cpu can operate faster than the much larger main memory .
most cpus since the 1980s have used one or more caches , sometimes in cascaded levels .
examples of caches with a specific function are the d-cache and i-cache and the translation lookaside buffer for the mmu .
it introduced morton order swizzled textures to improve 2d cache coherency .
cache misses would drastically affect performance , e . g . if mipmapping was not used .
caching was important to leverage 32-bitennial and widerncies for texture data that was often as little as 4 bits per pixel .
as gpus advanced cupping , they have developed progressively larger and increasingly general caches , including instruction caches for shaders , showing increasingly common functionality with cpu caches .
for example , gt200 architecture gpus did not feature an l2 cache , while the fermi gpu has 768 kb of last-level cache , the kepler gpu has 1536 kb of last-level cache , and the maxwell gpu has 2048 kb of last-level cache .
these caches have grown to handle synchronisation primitives between threads and atomic operations , and interface with a cpu-style mmu .
digital signal processors have also generalised over the years .
modern dsps such as qualcomm hexagon often include a set of caches to a cpuizable cpu cpu instruction with shared l2 , split l1 i-cache and d-cacheization .
a memory management unit saves mbu $ 1,000 from main memory .
this specialized cache is called a " lookaside buffer buffer buffer buffer armory " .
cpu caches are generally managed entirely by hardware , a variety of software manages other caches .
the operating system kernel is managed by the kernel .
the disk buffer , which is an integrated part of the hard disk drive , is sometimes called " disk cache " .
repeated cache hits are rare , due to the small size of the buffer in comparison to the drive 's capacity .
high-end disk controllers often have their own cache of hard disk drive 's data blocks .
this is the main idea of hierarchical storage management .
they can also be used as caches for slower rotational-media hard disk drives .
web browsers and web proxy servers use caches to store previous responses from web servers , such as web pages and images .
information previously stored in the cache can often be re-used .
this reduces bandwidth and processing requirements of the web server , and helps to improve responsiveness for users of the web .
web browsers employ a built-in web cache .
another form of cache is p2p caching .
similarly , decentralised equivalents exist , which allow communities to perform the same task for p2p traffic , for example corelli .
a cache can store data that is computed on demand rather than obtained from a backing store .
memoization is a way to store the results of resource-consuming function calls within a lookup table .
it is related to the dynamic programming algorithm design methodology .
the bind dns daemon caches a mapping of domain names to ip addresses , as does a resolver library .
write-through operation is common in unreliable networks involving the coherency protocol required between multiple write-back caches when communication is unreliable .
for instance , web page caches and client-side network file system caches are typically read-only or write-through specifically to keep the network protocol simple and reliable .
search engines also often make web pages they have indexed available from their cache .
for example , google gives a " cached " link next to each search result .
this can prove useful when web pages are temporarily inaccessible .
another type of caching is storing computed results that will likely be needed again , or memoization .
for example , ccache is a program that caches the output of the compilation , in order to speed up later compilation runs .
database caching can improve the throughput of database applications .
a distributed cache uses networked hosts to provide scalability , reliability and performance to the application .
the hosts can be moved over different regions .
the semantics of a " buffer " and a " cache " are not totally different .
caching realizes that the data is being transferred repeatedly .
while a caching system may realize a performance increase upon the initialization typically write ashore transfer of a data item , this performance increase is due to buffering occurring within the caching system .
with read caches , a data item must have been fetched from its residing location at least once in order for subsequent reads of the data item to realize a performance increase by virtue of being able to be fetched from the cache 's ( faster eki eki eki eki storage rather than the actual location .
write caches can be realized when the first write of the data item is immediately stored in the cache 's intermediate storage .
contrary to strict buffering , a caching process must adhere to ahend potentially distributed zambia cache coherency protocol in order to maintain consistency between the cache 's intermediate storage and the location where the data resides .
buffering , on the other hand , is buffering .
with typical caching implementations , a data item that is read or written for the first time is effectively buffered ; and in the case of a write , realizing a performance increase for the application from where the write originated .
additionally , the portion of a caching protocol is a form of buffering .
the portion of a caching protocol where individual reads are deferred to a batch of reads is also a form of buffering .
in practice , caching almost always involves some form of buffering , while strict buffering does not involve caching .
a buffer is a location in memory that is traditionally used because cpu instructionserie directly address data stored in peripheral devices .
memory memory is used as an intermediate store .
additionally , such a buffer may be feasible when a large block of data is assembled or disassembled manually as required by a storage device specifies .
also , a whole buffer of data is usually transferred sequentially abbreviated to hard disklets , so buffering itself sometimes increases transfer performance or reduces the variation or jitter of the transfer 's latency .
these benefits are present even if the buffered data are written to the buffer once and read from the buffer once .
a cache also increases transfer performance .
a part of the increase also comes from the possibility that multiple small transfers will combine into one large block .
but the main performance gain occurs because there is a good chance that the same data will be read from cache multiple times , or that written data will soon be read .
the only purpose of caches is to reduce accesses to the underlying slower storage .
cache is also a abstraction layer that is designed to be invisible from the perspective of neighboring layers .

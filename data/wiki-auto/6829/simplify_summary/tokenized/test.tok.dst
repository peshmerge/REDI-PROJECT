[CLS] in computing , a cache ( , or in au ##e ) is a hardware or software component that stores data so that future requests for that data can be served faster ; the data stored in a cache might be the result of an earlier computation or a copy of data stored elsewhere . [SEP]
[CLS] a " cache hit " occurs when the requested data can be found in a cache , while a " cache miss " occurs when it cannot . [SEP]
[CLS] cache hits are served by reading data from the cache , which is faster than rec ##omp ##uting a result or reading from a slower data store ; thus , the more requests that can be served from the cache , the faster the system performs . [SEP]
[CLS] to be cost - effective and to enable efficient use of data , cache ##s must be relatively small . [SEP]
[CLS] nevertheless , cache ##s have proven themselves in many areas of computing , because typical computer applications access data with a high degree of locality of reference . [SEP]
[CLS] such access patterns exhibit temporal locality , where data is requested that has been recently requested already , and spatial locality , where data is requested that is stored physically close to data that has already been requested . [SEP]
[CLS] there is an inherent trade - off between size and speed ( given that a larger resource implies greater physical distances ) but also a trade ##off between expensive , premium technologies ( such as sr ##am ) vs cheaper , easily mass - produced commodities ( such as dr ##am or hard disks ) . [SEP]
[CLS] a larger resource inc ##urs a significant late ##ncy for access e . g . it can take hundreds of clock cycles for a modern 4 ghz processor to reach dr ##am . [SEP]
[CLS] this is mit ##igate ##d by reading in large chunks , in the hope that subsequent reads will be from nearby locations . [SEP]
[CLS] prediction or explicit pre ##fe ##tch ##ing might also guess where future reads will come from and make requests ahead of time ; if done correctly the late ##ncy is bypassed altogether . [SEP]
[CLS] the use of a cache also allows for higher through ##put from the underlying resource , by ass ##em ##bling multiple fine grain transfers into larger , more efficient requests . [SEP]
[CLS] in the case of dr ##am circuits , this might be served by having a wider data bus . [SEP]
[CLS] for example , consider a program access ##ing bytes in a 32 - bit address space , but being served by a 128 - bit off - chip data bus ; individual un ##ca ##ched byte access ##es would allow only 1 / 16th of the total bandwidth to be used , and 80 % of the data movement would be memory addresses instead of data itself . [SEP]
[CLS] reading larger chunks reduces the fraction of bandwidth required for transmitting address information . [SEP]
[CLS] hardware implements cache as a block of memory for temporary storage of data likely to be used again . [SEP]
[CLS] central processing units ( cpu ##s ) and hard disk drives ( hd ##ds ) frequently use a cache , as do web browser ##s and web servers . [SEP]
[CLS] a cache is made up of a pool of entries . [SEP]
[CLS] each entry has associated " data " , which is a copy of the same data in some " backing store " . [SEP]
[CLS] each entry also has a " tag " , which specifies the identity of the data in the backing store of which the entry is a copy . [SEP]
[CLS] when the cache client ( a cpu , web browser , operating system ) needs to access data presumed to exist in the backing store , it first checks the cache . [SEP]
[CLS] if an entry can be found with a tag matching that of the desired data , the data in the entry is used instead . [SEP]
[CLS] this situation is known as a cache hit . [SEP]
[CLS] for example , a web browser program might check its local cache on disk to see if it has a local copy of the contents of a web page at a particular ur ##l . [SEP]
[CLS] in this example , the ur ##l is the tag , and the content of the web page is the data . [SEP]
[CLS] the percentage of access ##es that result in cache hits is known as the hit rate or hit ratio of the cache . [SEP]
[CLS] the alternative situation , when the cache is checked and found not to contain any entry with the desired tag , is known as a cache miss . [SEP]
[CLS] this requires a more expensive access of data from the backing store . [SEP]
[CLS] once the requested data is retrieved , it is typically copied into the cache , ready for the next access . [SEP]
[CLS] during a cache miss , some other previously existing cache entry is removed in order to make room for the newly retrieved data . [SEP]
[CLS] the he ##uri ##stic used to select the entry to replace is known as the replacement policy . [SEP]
[CLS] one popular replacement policy , " least recently used " ( l ##ru ) , replaces the oldest entry , the entry that was accessed less recently than any other entry ( see cache algorithm ) . [SEP]
[CLS] more efficient ca ##ching algorithms compute the use - hit frequency against the size of the stored contents , as well as the late ##ncies and through ##put ##s for both the cache and the backing store . [SEP]
[CLS] this works well for larger amounts of data , longer late ##ncies , and slower through ##put ##s , such as that experienced with hard drives and networks , but is not efficient for use within a cpu cache . [SEP]
[CLS] when a system writes data to cache , it must at some point write that data to the backing store as well . [SEP]
[CLS] the timing of this write is controlled by what is known as the " write policy " . [SEP]
[CLS] a write - back cache is more complex to implement , since it needs to track which of its locations have been written over , and mark them as " dirty " for later writing to the backing store . [SEP]
[CLS] the data in these locations are written back to the backing store only when they are evicted from the cache , an effect referred to as a " lazy write " . [SEP]
[CLS] for this reason , a read miss in a write - back cache ( which requires a block to be replaced by another ) will often require two memory access ##es to service : one to write the replaced data from the cache back to the store , and then one to retrieve the needed data . [SEP]
[CLS] other policies may also trigger data write - back . [SEP]
[CLS] the client may make many changes to data in the cache , and then explicitly not ##ify the cache to write back the data . [SEP]
[CLS] since no data is returned to the request ##er on write operations , a decision needs to be made on write misses , whether or not data would be loaded into the cache . [SEP]
[CLS] entities other than the cache may change the data in the backing store , in which case the copy in the cache may become out - of - date or " stale " . [SEP]
[CLS] alternatively , when the client updates the data in the cache , copies of those data in other cache ##s will become stale . [SEP]
[CLS] communication protocols between the cache managers which keep the data consistent are known as co ##her ##ency protocols . [SEP]
[CLS] small memories on or close to the cpu can operate faster than the much larger main memory . [SEP]
[CLS] most cpu ##s since the 1980s have used one or more cache ##s , sometimes in cascade ##d levels ; modern high - end embedded , desktop and server micro ##pro ##ces ##sor ##s may have as many as six types of cache ( between levels and functions ) . [SEP]
[CLS] examples of cache ##s with a specific function are the d - cache and i - cache and the translation look ##asi ##de buffer for the mm ##u . [SEP]
[CLS] earlier graphics processing units ( gp ##us ) often had limited read - only texture cache ##s , and introduced morton order sw ##iz ##zle ##d textures to improve 2d cache co ##her ##ency . [SEP]
[CLS] cache misses would drastically affect performance , e . g . if mi ##pm ##app ##ing was not used . [SEP]
[CLS] ca ##ching was important to leverage 32 - bit ( and wider ) transfers for texture data that was often as little as 4 bits per pixel , indexed in complex patterns by arbitrary uv coordinates and perspective transformations in inverse texture mapping . [SEP]
[CLS] as gp ##us advanced ( especially with gp ##gp ##u compute shade ##rs ) they have developed progressively larger and increasingly general cache ##s , including instruction cache ##s for shade ##rs , exhibiting increasingly common functionality with cpu cache ##s . [SEP]
[CLS] for example , gt ##200 architecture gp ##us did not feature an l ##2 cache , while the fe ##rmi gp ##u has 76 ##8 kb of last - level cache , the kepler gp ##u has 153 ##6 kb of last - level cache , and the maxwell gp ##u has 204 ##8 kb of last - level cache . [SEP]
[CLS] these cache ##s have grown to handle sync ##hr ##onis ##ation primitive ##s between threads and atomic operations , and interface with a cpu - style mm ##u . [SEP]
[CLS] digital signal processors have similarly general ##ised over the years . [SEP]
[CLS] earlier designs used scratch ##pad memory fed by d ##ma , but modern ds ##ps such as qu ##al ##com ##m he ##xa ##gon often include a very similar set of cache ##s to a cpu ( e . g . modified harvard architecture with shared l ##2 , split l ##1 i - cache and d - cache ) . [SEP]
[CLS] a memory management unit ( mm ##u ) that fetch ##es page table entries from main memory has a specialized cache , used for recording the results of virtual address to physical address translations . [SEP]
[CLS] this specialized cache is called a translation look ##asi ##de buffer ( t ##lb ) . [SEP]
[CLS] while cpu cache ##s are generally managed entirely by hardware , a variety of software manages other cache ##s . [SEP]
[CLS] the page cache in main memory , which is an example of disk cache , is managed by the operating system kernel . [SEP]
[CLS] while the disk buffer , which is an integrated part of the hard disk drive , is sometimes misleading ##ly referred to as " disk cache " , its main functions are write sequencing and read pre ##fe ##tch ##ing . [SEP]
[CLS] repeated cache hits are relatively rare , due to the small size of the buffer in comparison to the drive ' s capacity . [SEP]
[CLS] however , high - end disk controllers often have their own on - board cache of the hard disk drive ' s data blocks . [SEP]
[CLS] finally , a fast local hard disk drive can also cache information held on even slower data storage devices , such as remote servers ( web cache ) or local tape drives or optical ju ##ke ##box ##es ; such a scheme is the main concept of hierarchical storage management . [SEP]
[CLS] also , fast flash - based solid - state drives ( ss ##ds ) can be used as cache ##s for slower rotational - media hard disk drives , working together as hybrid drives or solid - state hybrid drives ( ss ##hd ##s ) . [SEP]
[CLS] web browser ##s and web proxy servers employ web cache ##s to store previous responses from web servers , such as web pages and images . [SEP]
[CLS] web cache ##s reduce the amount of information that needs to be transmitted across the network , as information previously stored in the cache can often be re - used . [SEP]
[CLS] this reduces bandwidth and processing requirements of the web server , and helps to improve responsive ##ness for users of the web . [SEP]
[CLS] web browser ##s employ a built - in web cache , but some internet service providers ( is ##ps ) or organizations also use a ca ##ching proxy server , which is a web cache that is shared among all users of that network . [SEP]
[CLS] another form of cache is p ##2 ##p ca ##ching , where the files most sought for by peer - to - peer applications are stored in an is ##p cache to accelerate p ##2 ##p transfers . [SEP]
[CLS] similarly , decent ##ral ##ised equivalent ##s exist , which allow communities to perform the same task for p ##2 ##p traffic , for example , core ##lli . [SEP]
[CLS] a cache can store data that is computed on demand rather than retrieved from a backing store . [SEP]
[CLS] memo ##ization is an optimization technique that stores the results of resource - consuming function calls within a look ##up table , allowing subsequent calls to re ##use the stored results and avoid repeated computation . [SEP]
[CLS] it is related to the dynamic programming algorithm design methodology , which can also be thought of as a means of ca ##ching . [SEP]
[CLS] the bind d ##ns daemon cache ##s a mapping of domain names to ip addresses , as does a resolve ##r library . [SEP]
[CLS] write - through operation is common when operating over unreliable networks ( like an ethernet lan ) , because of the enormous complexity of the co ##her ##ency protocol required between multiple write - back cache ##s when communication is unreliable . [SEP]
[CLS] for instance , web page cache ##s and client - side network file system cache ##s ( like those in n ##fs or sm ##b ) are typically read - only or write - through specifically to keep the network protocol simple and reliable . [SEP]
[CLS] search engines also frequently make web pages they have indexed available from their cache . [SEP]
[CLS] for example , google provides a " cache ##d " link next to each search result . [SEP]
[CLS] this can prove useful when web pages from a web server are temporarily or permanently inaccessible . [SEP]
[CLS] another type of ca ##ching is storing computed results that will likely be needed again , or memo ##ization . [SEP]
[CLS] for example , cc ##ache is a program that cache ##s the output of the compilation , in order to speed up later compilation runs . [SEP]
[CLS] database ca ##ching can substantially improve the through ##put of database applications , for example in the processing of index ##es , data di ##ction ##aries , and frequently used subset ##s of data . [SEP]
[CLS] a distributed cache uses network ##ed hosts to provide scala ##bility , reliability and performance to the application . [SEP]
[CLS] the hosts can be co - located or spread over different geographical regions . [SEP]
[CLS] the semantics of a " buffer " and a " cache " are not totally different ; even so , there are fundamental differences in intent between the process of ca ##ching and the process of buffer ##ing . [SEP]
[CLS] fundamentally , ca ##ching realizes a performance increase for transfers of data that is being repeatedly transferred . [SEP]
[CLS] while a ca ##ching system may realize a performance increase upon the initial ( typically write ) transfer of a data item , this performance increase is due to buffer ##ing occurring within the ca ##ching system . [SEP]
[CLS] with read cache ##s , a data item must have been fetch ##ed from its residing location at least once in order for subsequent reads of the data item to realize a performance increase by virtue of being able to be fetch ##ed from the cache ' s ( faster ) intermediate storage rather than the data ' s residing location . [SEP]
[CLS] with write cache ##s , a performance increase of writing a data item may be realized upon the first write of the data item by virtue of the data item immediately being stored in the cache ' s intermediate storage , def ##er ##ring the transfer of the data item to its residing storage at a later stage or else occurring as a background process . [SEP]
[CLS] contrary to strict buffer ##ing , a ca ##ching process must adhere to a ( potentially distributed ) cache co ##her ##ency protocol in order to maintain consistency between the cache ' s intermediate storage and the location where the data resides . [SEP]
[CLS] buffer ##ing , on the other hand , [SEP]
[CLS] with typical ca ##ching implementations , a data item that is read or written for the first time is effectively being buffer ##ed ; and in the case of a write , mostly realizing a performance increase for the application from where the write originated . [SEP]
[CLS] additionally , the portion of a ca ##ching protocol where individual writes are def ##erre ##d to a batch of writes is a form of buffer ##ing . [SEP]
[CLS] the portion of a ca ##ching protocol where individual reads are def ##erre ##d to a batch of reads is also a form of buffer ##ing , although this form may negatively impact the performance of at least the initial reads ( even though it may positively impact the performance of the sum of the individual reads ) . [SEP]
[CLS] in practice , ca ##ching almost always involves some form of buffer ##ing , while strict buffer ##ing does not involve ca ##ching . [SEP]
[CLS] a buffer is a temporary memory location that is traditionally used because cpu instructions cannot directly address data stored in peripheral devices . [SEP]
[CLS] thus , address ##able memory is used as an intermediate stage . [SEP]
[CLS] additionally , such a buffer may be feasible when a large block of data is assembled or di ##sas ##se ##mbled ( as required by a storage device ) , or when data may be delivered in a different order than that in which it is produced . [SEP]
[CLS] also , a whole buffer of data is usually transferred sequential ##ly ( for example to hard disk ) , so buffer ##ing itself sometimes increases transfer performance or reduces the variation or ji ##tter of the transfer ' s late ##ncy as opposed to ca ##ching where the intent is to reduce the late ##ncy . [SEP]
[CLS] these benefits are present even if the buffer ##ed data are written to the buffer once and read from the buffer once . [SEP]
[CLS] a cache also increases transfer performance . [SEP]
[CLS] a part of the increase similarly comes from the possibility that multiple small transfers will combine into one large block . [SEP]
[CLS] but the main performance - gain occurs because there is a good chance that the same data will be read from cache multiple times , or that written data will soon be read . [SEP]
[CLS] a cache ' s sole purpose is to reduce access ##es to the underlying slower storage . [SEP]
[CLS] cache is also usually an abstraction layer that is designed to be invisible from the perspective of neighboring layers . [SEP]

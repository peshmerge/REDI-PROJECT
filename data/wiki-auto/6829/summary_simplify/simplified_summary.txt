in computing , a cache cache might be the result of an earlier computation or a copy of data stored elsewhere .
cache hits are served by reading data from the cache , which is faster than recomputing a result or reading from a slower data store .
caches have proven themselves in many areas of computing , because typical computer applications access data with a high degree of reference .
such access patterns show temporal locality , where data is requested that has been recently requested already , and spatial locality , where data is requested that is stored physically close to data that has already been requested .
there is also a tradeoff between expensive , expensive , premium technologies such as sram kapoor vs cheaper , easily mass-produced commodities such as dram or hard disks ashore .
this is mitigated by reading in large chunks , in the hope that subsequent reads will be from nearby locations .
prediction or explicit prefetching might also guess where future reads will come from and make requests ahead of time ; if done correctly the latency is bypassed altogether .
for example , consider a program accessing bytes in a 32-bit address space , but being served by a 128-bit off-chip data bus ; individual uncached byte accesses would allow only 1 / 16th of the total bandwidth to be used , and 80 % of the data movement would be memory instead of data itself .
cache is a block of memory for storing data that is likely used again .
central processing units aren cpus aground and hard disk drives libertarian hddsdsdsds employ cache , as do web browsers and web servers .
when the cache client learns a cpu , web browser , operating systemdner needs to access data in the backing store , it first checks the cache .
the percentage of cache hits are known as the hit rate or hit ratio of the cache .
this requires a more expensive access of data from the backing store .
once the requested data is found , it is typically copied into the cache , ready for the next time .
during cache miss , some other previously existing cache entry is removed in order to make room for the newly retrieved data .
the heuristic used to select the entry to replace replace replace replace replace replace replace is known as replacement policy .
more efficient caching algorithms compute the use-hit frequency against the size of the stored contents , as well as the latencies and throughputs for both the cache and the backing store .
it works well for larger amounts of data , longer latencies , and slower throughputs , such as that experienced with hard drives and networks , but is not efficient for use in cpu caches .
a write-back cache is more complex to implement .
the datum is written back to the backing store only when they are removed from the cache .
for this reason , a read miss in a write-back cache will often require two memory accesses to service : one to write the replaced data from the cache back to the store , and then one to get the needed data .
since no data is returned to the requester on write operations , a decision needs to be made on write misses , whether or not data would be loaded into the cache .
small memories on or close to the cpu can operate faster than the much larger main memory .
most cpus since the 1980s have used one or more caches , sometimes in cascaded levels .
it introduced morton order swizzled textures to improve 2d cache coherency .
these caches have grown to handle synchronisation primitives between threads and atomic operations , and interface with a cpu-style mmu .
modern dsps such as qualcomm hexagon often include a similar set of caches to a cpuizable .
a memory management unit saves mbu $ 1,000 from main memory .
cpu caches are generally managed entirely by hardware , a variety of software manages other caches .
the operating system kernel is managed by the kernel .
the disk buffer , which is an integrated part of the hard disk drive , is sometimes called " disk cache " .
repeated cache hits are rare , due to the small size of the buffer in comparison to the drive 's capacity .
this is the main idea of hierarchical storage management .
information previously stored in the cache can often be re-used .
web browsers employ a built-in web cache .
another form of cache is p2p caching .
a cache can store data that is computed on demand rather than obtained from a backing store .
the bind dns daemon caches a mapping of domain names to ip addresses , as does a resolver library .
for instance , web page caches and client-side network file system caches are typically read-only or write-through specifically to keep the network protocol simple and reliable .
another type of caching is storing computed results that will likely be needed again , or memoization .
database caching can improve the throughput of database applications .
a distributed cache uses networked hosts to provide scalability , reliability and performance to the application .
the hosts can be moved over different regions .
while a caching system may realize a performance increase upon the initialization typically write ashore transfer of a data item , this performance increase is due to buffering occurring within the caching system .
with read caches , a data item must have been fetched from its residing location at least once in order for subsequent reads of the data item to realize a performance increase by virtue of being able to be fetched from the cache 's ( faster eki eki eki eki storage rather than the actual location .
write caches can be realized when the first write of the data item is immediately stored in the cache 's intermediate storage .
contrary to strict buffering , a caching process must adhere to ahend potentially distributed zambia cache coherency protocol in order to maintain consistency between the cache 's intermediate storage and the location where the data resides .
buffering , on the other hand , is buffering .
with typical caching implementations , a data item that is read or written for the first time is effectively buffered ; and in the case of a write , realizing a performance increase for the application from where the write originated .
the portion of a caching protocol where individual reads are deferred to a batch of reads is also a form of buffering .
a buffer is a location in memory that is traditionally used because cpu instructionserie directly address data stored in peripheral devices .
additionally , such a buffer may be feasible when a large block of data is assembled or disassembled manually as required by a storage device specifies .
also , a whole buffer of data is usually transferred sequentially abbreviated to hard disklets , so buffering itself sometimes increases transfer performance or reduces the variation or jitter of the transfer 's latency .
but the main performance gain occurs because there is a good chance that the same data will be read from cache multiple times , or that written data will soon be read .
cache is also a abstraction layer that is designed to be invisible from the perspective of neighboring layers .

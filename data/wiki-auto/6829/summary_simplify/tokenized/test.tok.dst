[CLS] in computing , a cache ( , or in au ##e ) is a hardware or software component that stores data so that future requests for that data can be served faster ; the data stored in a cache might be the result of an earlier computation or a copy of data stored elsewhere . [SEP]
[CLS] cache hits are served by reading data from the cache , which is faster than rec ##omp ##uting a result or reading from a slower data store ; thus , the more requests that can be served from the cache , the faster the system performs . [SEP]
[CLS] nevertheless , cache ##s have proven themselves in many areas of computing , because typical computer applications access data with a high degree of locality of reference . [SEP]
[CLS] such access patterns exhibit temporal locality , where data is requested that has been recently requested already , and spatial locality , where data is requested that is stored physically close to data that has already been requested . [SEP]
[CLS] there is an inherent trade - off between size and speed ( given that a larger resource implies greater physical distances ) but also a trade ##off between expensive , premium technologies ( such as sr ##am ) vs cheaper , easily mass - produced commodities ( such as dr ##am or hard disks ) . [SEP]
[CLS] this is mit ##igate ##d by reading in large chunks , in the hope that subsequent reads will be from nearby locations . [SEP]
[CLS] prediction or explicit pre ##fe ##tch ##ing might also guess where future reads will come from and make requests ahead of time ; if done correctly the late ##ncy is bypassed altogether . [SEP]
[CLS] for example , consider a program access ##ing bytes in a 32 - bit address space , but being served by a 128 - bit off - chip data bus ; individual un ##ca ##ched byte access ##es would allow only 1 / 16th of the total bandwidth to be used , and 80 % of the data movement would be memory addresses instead of data itself . [SEP]
[CLS] hardware implements cache as a block of memory for temporary storage of data likely to be used again . [SEP]
[CLS] central processing units ( cpu ##s ) and hard disk drives ( hd ##ds ) frequently use a cache , as do web browser ##s and web servers . [SEP]
[CLS] when the cache client ( a cpu , web browser , operating system ) needs to access data presumed to exist in the backing store , it first checks the cache . [SEP]
[CLS] the percentage of access ##es that result in cache hits is known as the hit rate or hit ratio of the cache . [SEP]
[CLS] this requires a more expensive access of data from the backing store . [SEP]
[CLS] once the requested data is retrieved , it is typically copied into the cache , ready for the next access . [SEP]
[CLS] during a cache miss , some other previously existing cache entry is removed in order to make room for the newly retrieved data . [SEP]
[CLS] the he ##uri ##stic used to select the entry to replace is known as the replacement policy . [SEP]
[CLS] more efficient ca ##ching algorithms compute the use - hit frequency against the size of the stored contents , as well as the late ##ncies and through ##put ##s for both the cache and the backing store . [SEP]
[CLS] this works well for larger amounts of data , longer late ##ncies , and slower through ##put ##s , such as that experienced with hard drives and networks , but is not efficient for use within a cpu cache . [SEP]
[CLS] a write - back cache is more complex to implement , since it needs to track which of its locations have been written over , and mark them as " dirty " for later writing to the backing store . [SEP]
[CLS] the data in these locations are written back to the backing store only when they are evicted from the cache , an effect referred to as a " lazy write " . [SEP]
[CLS] for this reason , a read miss in a write - back cache ( which requires a block to be replaced by another ) will often require two memory access ##es to service : one to write the replaced data from the cache back to the store , and then one to retrieve the needed data . [SEP]
[CLS] since no data is returned to the request ##er on write operations , a decision needs to be made on write misses , whether or not data would be loaded into the cache . [SEP]
[CLS] small memories on or close to the cpu can operate faster than the much larger main memory . [SEP]
[CLS] most cpu ##s since the 1980s have used one or more cache ##s , sometimes in cascade ##d levels ; modern high - end embedded , desktop and server micro ##pro ##ces ##sor ##s may have as many as six types of cache ( between levels and functions ) . [SEP]
[CLS] earlier graphics processing units ( gp ##us ) often had limited read - only texture cache ##s , and introduced morton order sw ##iz ##zle ##d textures to improve 2d cache co ##her ##ency . [SEP]
[CLS] these cache ##s have grown to handle sync ##hr ##onis ##ation primitive ##s between threads and atomic operations , and interface with a cpu - style mm ##u . [SEP]
[CLS] earlier designs used scratch ##pad memory fed by d ##ma , but modern ds ##ps such as qu ##al ##com ##m he ##xa ##gon often include a very similar set of cache ##s to a cpu ( e . g . [SEP]
[CLS] a memory management unit ( mm ##u ) that fetch ##es page table entries from main memory has a specialized cache , used for recording the results of virtual address to physical address translations . [SEP]
[CLS] while cpu cache ##s are generally managed entirely by hardware , a variety of software manages other cache ##s . [SEP]
[CLS] the page cache in main memory , which is an example of disk cache , is managed by the operating system kernel . [SEP]
[CLS] while the disk buffer , which is an integrated part of the hard disk drive , is sometimes misleading ##ly referred to as " disk cache " , its main functions are write sequencing and read pre ##fe ##tch ##ing . [SEP]
[CLS] repeated cache hits are relatively rare , due to the small size of the buffer in comparison to the drive ' s capacity . [SEP]
[CLS] finally , a fast local hard disk drive can also cache information held on even slower data storage devices , such as remote servers ( web cache ) or local tape drives or optical ju ##ke ##box ##es ; such a scheme is the main concept of hierarchical storage management . [SEP]
[CLS] web cache ##s reduce the amount of information that needs to be transmitted across the network , as information previously stored in the cache can often be re - used . [SEP]
[CLS] web browser ##s employ a built - in web cache , but some internet service providers ( is ##ps ) or organizations also use a ca ##ching proxy server , which is a web cache that is shared among all users of that network . [SEP]
[CLS] another form of cache is p ##2 ##p ca ##ching , where the files most sought for by peer - to - peer applications are stored in an is ##p cache to accelerate p ##2 ##p transfers . [SEP]
[CLS] a cache can store data that is computed on demand rather than retrieved from a backing store . [SEP]
[CLS] the bind d ##ns daemon cache ##s a mapping of domain names to ip addresses , as does a resolve ##r library . [SEP]
[CLS] for instance , web page cache ##s and client - side network file system cache ##s ( like those in n ##fs or sm ##b ) are typically read - only or write - through specifically to keep the network protocol simple and reliable . [SEP]
[CLS] another type of ca ##ching is storing computed results that will likely be needed again , or memo ##ization . [SEP]
[CLS] database ca ##ching can substantially improve the through ##put of database applications , for example in the processing of index ##es , data di ##ction ##aries , and frequently used subset ##s of data . [SEP]
[CLS] a distributed cache uses network ##ed hosts to provide scala ##bility , reliability and performance to the application . [SEP]
[CLS] the hosts can be co - located or spread over different geographical regions . [SEP]
[CLS] while a ca ##ching system may realize a performance increase upon the initial ( typically write ) transfer of a data item , this performance increase is due to buffer ##ing occurring within the ca ##ching system . [SEP]
[CLS] with read cache ##s , a data item must have been fetch ##ed from its residing location at least once in order for subsequent reads of the data item to realize a performance increase by virtue of being able to be fetch ##ed from the cache ' s ( faster ) intermediate storage rather than the data ' s residing location . [SEP]
[CLS] with write cache ##s , a performance increase of writing a data item may be realized upon the first write of the data item by virtue of the data item immediately being stored in the cache ' s intermediate storage , def ##er ##ring the transfer of the data item to its residing storage at a later stage or else occurring as a background process . [SEP]
[CLS] contrary to strict buffer ##ing , a ca ##ching process must adhere to a ( potentially distributed ) cache co ##her ##ency protocol in order to maintain consistency between the cache ' s intermediate storage and the location where the data resides . [SEP]
[CLS] buffer ##ing , on the other hand , [SEP]
[CLS] with typical ca ##ching implementations , a data item that is read or written for the first time is effectively being buffer ##ed ; and in the case of a write , mostly realizing a performance increase for the application from where the write originated . [SEP]
[CLS] the portion of a ca ##ching protocol where individual reads are def ##erre ##d to a batch of reads is also a form of buffer ##ing , although this form may negatively impact the performance of at least the initial reads ( even though it may positively impact the performance of the sum of the individual reads ) . [SEP]
[CLS] a buffer is a temporary memory location that is traditionally used because cpu instructions cannot directly address data stored in peripheral devices . [SEP]
[CLS] additionally , such a buffer may be feasible when a large block of data is assembled or di ##sas ##se ##mbled ( as required by a storage device ) , or when data may be delivered in a different order than that in which it is produced . [SEP]
[CLS] also , a whole buffer of data is usually transferred sequential ##ly ( for example to hard disk ) , so buffer ##ing itself sometimes increases transfer performance or reduces the variation or ji ##tter of the transfer ' s late ##ncy as opposed to ca ##ching where the intent is to reduce the late ##ncy . [SEP]
[CLS] but the main performance - gain occurs because there is a good chance that the same data will be read from cache multiple times , or that written data will soon be read . [SEP]
[CLS] cache is also usually an abstraction layer that is designed to be invisible from the perspective of neighboring layers . [SEP]

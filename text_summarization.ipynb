{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Simple Text summarization- using unsupervised learning\n",
    "\n",
    "###  Given any Input document → sentences similarity → weight sentences → select sentences with higher rank.\n",
    "#### Input article → split into sentences → remove stop words → build a similarity matrix → generate rank based on matrix → pick top N sentences for summary.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "# !pip install cdifflib\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    return nltk.tokenize.sent_tokenize(file.read())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  similarity matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    "\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    "\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "# A function to reorder the sentence to the original order in the original text. Not used at the moment, but could be used for debugging. Remove before release\n",
    "def reorder_sentences(original_sents, summarization_sents):\n",
    "    temp_dict = dict()\n",
    "    for sentence in summarization_sents:\n",
    "        if sentence in original_sents:\n",
    "            temp_dict[original_sents.index(sentence)] = sentence\n",
    "    return list((dict(sorted(temp_dict.items())).values()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating a summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# original sentences 22\n",
      "# summarized sentences 12\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<strong>Full Text</strong><br><span style=\"background-color:rgba(255,215,0,0.3);\">The Galactic RainCloudS project, an initiative led by members of the Faculty of Physics, the Institute of Cosmos Sciences (ICCUB) of the University of Barcelona and the Institute for Space Studies of Catalonia (IEEC), was awarded the first position in the framework of the Cloud Funding for Research call of the European project Open Clouds For Research Environments (OCRE). </span> The project competed against 27 proposals from twelve countries in a wide range of research disciplines. This first edition of Cloud Funding For Research funds the use of commercial computational cloud resources for research. The project counts on the collaboration from the private sector, and specifically from Pervasive Technologies, which brings experience in artificial intelligence and cloud computing; Google, and the computing infrastructure from Google Cloud and Telefónica, which offers experience on cloud resource management. <span style=\"background-color:rgba(255,215,0,0.3);\">Professor Xavier Luri, director of ICCUB and principal researcher of the project, highlights that “The Galactic RainCloudS project is a pioneer one in Europe in the use of commercial cloud infrastructures for research on astronomy, and results from the will to show the benefits of cloud resource uses for the scientific community”</span> . The key of the project lies in interdisciplinarity: combining the extraordinary volumes of data from the European Space Agency Gaia Satellite with the great computational power and the flexibility of cloud infrastructures, and with the data mining techniques, it will enable the team of the University of Barcelona to study the existing links between past galaxy collisions and star formation in a holistic way, a study in which the Milky Way and satellite galaxies will be an experimental laboratory <span style=\"background-color:rgba(255,215,0,0.3);\">. “Cloud computing is like renting powerful customized computers, for a certain period of time, which will enable us to make the necessary calculations to study the interaction between galaxies”, notes Mercè Romero, researcher at ICCUB. T</span> he project also includes the development of a system to detect traces of past small galaxy collisions with the halo of our galaxy. T <span style=\"background-color:rgba(255,215,0,0.3);\">eresa Antoja, researcher at ICCUB, notes that “the existence of granularities in the galactic halos is a prediction of the current cosmological model of the formation of our Universe: the active search for substructures of this type in the Gaia data can provide vital information on the history of the Milky Way and on the nature of dark matter”. </span> The participation of the private sector in this project shows the closeness between research and companies in the use of cutting-edge technologies as well as their shared interests. <span style=\"background-color:rgba(255,215,0,0.3);\">“In Pervasive Technologies, we are glad to offer our knowledge on artificial intelligence and cloud computing to a pioneer project in the field of research. We will work to get the highest performance of the cloud infrastructures and artificial intelligence for this project”, notes Rodolfo Lomascolo, CEO of Pervasive Technologies. </span> In order to be successful, the Galactic RainCloudS project must have, among other features, big data infrastructures. “The Gaia satellite data hide the answer to many questions we want to solve, but we need the right tools to retrieve them”, notes Roger Mor, data scientist at Pervasive Technologies and ICCUB collaborator. <span style=\"background-color:rgba(255,215,0,0.3);\">He adds: “The available big data platforms in the commercial cloud and artificial intelligence services are fundamental tools to find, for instance, whether the interaction of Sagittarius with the Milky Way caused the reignition of the star formation in our galaxy between 5 and 7billion years ago, as stated in some studies”.</span> <span style=\"background-color:rgba(255,215,0,0.3);\">Enrique González Lezana, head of cloud sales specialist at Telefónica Tech, says that “Telefónica has accompanied the University of Barcelona in the definition and unfolding of the Google Cloud architecture, where the required hypercomputing solution to work on the Galactic RainCloudS project will be hosted”.</span> <span style=\"background-color:rgba(255,215,0,0.3);\">“The unfolded infrastructure —he adds— will enable the processing and analysis of big data in a flexible, scalable way, adjusted to the required needs of the researchers of the University of Barcelona.</span> <span style=\"background-color:rgba(255,215,0,0.3);\">Telefónica will work with the UB during the entire process to guarantee the successful implementation of the project with teams specialized on Google Cloud services and technologies”.</span> The project launched this May and will last a year. <span style=\"background-color:rgba(255,215,0,0.3);\">“Galactic RainCLoudS is a necessary step in the transition of the world of research toward the efficient use of cloud computing resources.</span> In this sense, we are pioneers in its use at the University of Barcelona and we hope our experience serves to encourage its use. <span style=\"background-color:rgba(255,215,0,0.3);\">The research teams’ needs are becoming more specific, and we are making an effort for this project to open the doors of commercial cloud computing in future projects for all research disciplines”, concludes Xavier Luri.</span><br><br><strong>Predicted Summary</strong><br><span style=\"background-color:rgba(255,215,0,0.3);\">The Galactic RainCloudS project, an initiative led by members of the Faculty of Physics, the Institute of Cosmos Sciences (ICCUB) of the University of Barcelona and the Institute for Space Studies of Catalonia (IEEC), was awarded the first position in the framework of the Cloud Funding for Research call of the European project Open Clouds For Research Environments (OCRE). </span> <span style=\"background-color:rgba(255,215,0,0.3);\">Professor Xavier Luri, director of ICCUB and principal researcher of the project, highlights that “The Galactic RainCloudS project is a pioneer one in Europe in the use of commercial cloud infrastructures for research on astronomy, and results from the will to show the benefits of cloud resource uses for the scientific community”</span> <span style=\"background-color:rgba(255,215,0,0.3);\">. “Cloud computing is like renting powerful customized computers, for a certain period of time, which will enable us to make the necessary calculations to study the interaction between galaxies”, notes Mercè Romero, researcher at ICCUB. T</span> <span style=\"background-color:rgba(255,215,0,0.3);\">eresa Antoja, researcher at ICCUB, notes that “the existence of granularities in the galactic halos is a prediction of the current cosmological model of the formation of our Universe: the active search for substructures of this type in the Gaia data can provide vital information on the history of the Milky Way and on the nature of dark matter”. </span> <span style=\"background-color:rgba(255,215,0,0.3);\">“In Pervasive Technologies, we are glad to offer our knowledge on artificial intelligence and cloud computing to a pioneer project in the field of research. We will work to get the highest performance of the cloud infrastructures and artificial intelligence for this project”, notes Rodolfo Lomascolo, CEO of Pervasive Technologies. </span> <span style=\"background-color:rgba(255,215,0,0.3);\">He adds: “The available big data platforms in the commercial cloud and artificial intelligence services are fundamental tools to find, for instance, whether the interaction of Sagittarius with the Milky Way caused the reignition of the star formation in our galaxy between 5 and 7billion years ago, as stated in some studies”.</span> <span style=\"background-color:rgba(255,215,0,0.3);\">Enrique González Lezana, head of cloud sales specialist at Telefónica Tech, says that “Telefónica has accompanied the University of Barcelona in the definition and unfolding of the Google Cloud architecture, where the required hypercomputing solution to work on the Galactic RainCloudS project will be hosted”.</span> <span style=\"background-color:rgba(255,215,0,0.3);\">“The unfolded infrastructure —he adds— will enable the processing and analysis of big data in a flexible, scalable way, adjusted to the required needs of the researchers of the University of Barcelona.</span> <span style=\"background-color:rgba(255,215,0,0.3);\">Telefónica will work with the UB during the entire process to guarantee the successful implementation of the project with teams specialized on Google Cloud services and technologies”.</span> <span style=\"background-color:rgba(255,215,0,0.3);\">“Galactic RainCLoudS is a necessary step in the transition of the world of research toward the efficient use of cloud computing resources.</span> <span style=\"background-color:rgba(255,215,0,0.3);\">The research teams’ needs are becoming more specific, and we are making an effort for this project to open the doors of commercial cloud computing in future projects for all research disciplines”, concludes Xavier Luri.</span>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_summary(file_name):\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    sentences =  read_file(file_name)\n",
    "\n",
    "    # The number of sentences in the summarization\n",
    "    ranked_sentences_total = (int)(len(sentences)/2)+1\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    rnk_sent_with_score = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "\n",
    "    print(\"# original sentences\" , len(sentences))\n",
    "    print(\"# summarized sentences\" , (int)(len(sentences)/2)+1)\n",
    "\n",
    "    # Restore the original sentence order\n",
    "    ranked_sentences=list(dict(sorted((dict(map((lambda sentence: (\n",
    "        sentences.index(sentence),sentence) if sentence in sentences else (None,sentence)), dict(rnk_sent_with_score[:ranked_sentences_total]).values()))).items())).values())\n",
    "    match = display_string_matching(\" \".join(sentences), \" \".join(ranked_sentences), both=True, sentences=True, titles=[\"Full Text\", \"Predicted Summary\"])\n",
    "\n",
    "    display(HTML(match))\n",
    "    # return \" \".join(ranked_sentences)\n",
    "\n",
    "generate_summary( \"cloud_computing_eurekalert.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "#Code used https://towardsdatascience.com/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09\n",
    "'''\n",
    "Find the matching substrings in 2 strings.\n",
    ":parameter\n",
    "    :param a: string - raw text\n",
    "    :param b: string - raw text\n",
    ":return\n",
    "    2 lists used in to display matches\n",
    "'''\n",
    "def utils_split_sentences(a, b):\n",
    "    ## find clean matches\n",
    "    match = difflib.SequenceMatcher(isjunk=None, a=a, b=b, autojunk=True)\n",
    "    lst_match = [block for block in match.get_matching_blocks() if block.size > 20]\n",
    "\n",
    "    ## difflib didn't find any match\n",
    "    if len(lst_match) == 0:\n",
    "        lst_a, lst_b = nltk.sent_tokenize(a), nltk.sent_tokenize(b)\n",
    "\n",
    "    ## work with matches\n",
    "    else:\n",
    "        first_m, last_m = lst_match[0], lst_match[-1]\n",
    "\n",
    "        ### a\n",
    "        string = a[0 : first_m.a]\n",
    "        lst_a = [t for t in nltk.sent_tokenize(string)]\n",
    "        for n in range(len(lst_match)):\n",
    "            m = lst_match[n]\n",
    "            string = a[m.a : m.a+m.size]\n",
    "            lst_a.append(string)\n",
    "            if n+1 < len(lst_match):\n",
    "                next_m = lst_match[n+1]\n",
    "                string = a[m.a+m.size : next_m.a]\n",
    "                lst_a = lst_a + [t for t in nltk.sent_tokenize(string)]\n",
    "            else:\n",
    "                break\n",
    "        string = a[last_m.a+last_m.size :]\n",
    "        lst_a = lst_a + [t for t in nltk.sent_tokenize(string)]\n",
    "\n",
    "        ### b\n",
    "        string = b[0 : first_m.b]\n",
    "        lst_b = [t for t in nltk.sent_tokenize(string)]\n",
    "        for n in range(len(lst_match)):\n",
    "            m = lst_match[n]\n",
    "            string = b[m.b : m.b+m.size]\n",
    "            lst_b.append(string)\n",
    "            if n+1 < len(lst_match):\n",
    "                next_m = lst_match[n+1]\n",
    "                string = b[m.b+m.size : next_m.b]\n",
    "                lst_b = lst_b + [t for t in nltk.sent_tokenize(string)]\n",
    "            else:\n",
    "                break\n",
    "        string = b[last_m.b+last_m.size :]\n",
    "        lst_b = lst_b + [t for t in nltk.sent_tokenize(string)]\n",
    "\n",
    "    return lst_a, lst_b\n",
    "\n",
    "'''\n",
    "Highlights the matched strings in text.\n",
    ":parameter\n",
    "    :param a: string - raw text\n",
    "    :param b: string - raw text\n",
    "    :param both: bool - search a in b and, if True, viceversa\n",
    "    :param sentences: bool - if False matches single words\n",
    ":return\n",
    "    text html, it can be visualized on notebook with display(HTML(text))\n",
    "'''\n",
    "def display_string_matching(a, b, both=True, sentences=True, titles=[]):\n",
    "    if sentences is True:\n",
    "        lst_a, lst_b = utils_split_sentences(a, b)\n",
    "    else:\n",
    "        lst_a, lst_b = a.split(), b.split()\n",
    "\n",
    "        ## highlight a\n",
    "    first_text = []\n",
    "    for i in lst_a:\n",
    "        if re.sub(r'[^\\w\\s]', '', i.lower()) in [re.sub(r'[^\\w\\s]', '', z.lower()) for z in lst_b]:\n",
    "            first_text.append('<span style=\"background-color:rgba(255,215,0,0.3);\">' + i + '</span>')\n",
    "        else:\n",
    "            first_text.append(i)\n",
    "    first_text = ' '.join(first_text)\n",
    "\n",
    "    ## highlight b\n",
    "    second_text = []\n",
    "    if both is True:\n",
    "        for i in lst_b:\n",
    "            if re.sub(r'[^\\w\\s]', '', i.lower()) in [re.sub(r'[^\\w\\s]', '', z.lower()) for z in lst_a]:\n",
    "                second_text.append('<span style=\"background-color:rgba(255,215,0,0.3);\">' + i + '</span>')\n",
    "            else:\n",
    "                second_text.append(i)\n",
    "    else:\n",
    "        second_text.append(b)\n",
    "    second_text = ' '.join(second_text)\n",
    "\n",
    "    ## concatenate\n",
    "    if len(titles) > 0:\n",
    "        first_text = \"<strong>\"+titles[0]+\"</strong><br>\"+first_text\n",
    "    if len(titles) > 1:\n",
    "        second_text = \"<strong>\"+titles[1]+\"</strong><br>\"+second_text\n",
    "    else:\n",
    "        second_text = \"---\"*65+\"<br><br>\"+second_text\n",
    "    final_text = first_text +'<br><br>'+ second_text\n",
    "    return final_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
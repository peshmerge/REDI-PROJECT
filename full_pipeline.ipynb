{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "# !pip install cdifflib\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define required methods\n",
    "\n",
    "Taken from `text_simplification.ipynb` and `simplified_summary_pipeline.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess test_data/raw test_data/tokenized test_data/binarized\n",
    "def preprocess_data(raw_data_dir, tokenized_data_dir, binarized_data_dir):\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/test.src --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/test.tok.src\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/test.dst --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/test.tok.dst \n",
    "\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/valid.src --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/valid.tok.src \n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/valid.dst --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/valid.tok.dst \n",
    "\n",
    "\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/train.src --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/train.tok.src \n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/train.dst --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/train.tok.dst \n",
    "\n",
    "\n",
    "    # Creates binarized fairseq dataset\n",
    "    !python simplification/preprocess.py --workers 5 --source-lang src --target-lang dst --trainpref $tokenized_data_dir/train.tok --validpref $tokenized_data_dir/valid.tok --testpref $tokenized_data_dir/test.tok --destdir  $binarized_data_dir --padding-factor 1 --joined-dictionary --srcdict simplification/preprocess/vocab_count.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simplified_text(binarized_data_dir, output_file, checkpoint_file, gpu_id=\"\", split=\"test\"):\n",
    "    !export CUDA_VISIBLE_DEVICES=$gpu_id\n",
    "    !python simplification/generate.py $binarized_data_dir --path $checkpoint_file --batch-size 32  --beam 1 --nbest 1 --user-dir simplification/my_model/ --print-alignment --gen-subset $split > $output_file'.aner'\n",
    "\n",
    "    !python simplification/postprocess/bpe.py  --out_anon $output_file'.aner' --denon $output_file --ignore_lines 5 --wp 1\n",
    "\n",
    "    !rm $output_file'.aner'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    return nltk.tokenize.sent_tokenize(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(file_name):\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    sentences =  read_file(file_name)\n",
    "\n",
    "    # The number of sentences in the summarization\n",
    "    ranked_sentences_total = (len(sentences)//2)+1\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    rnk_sent_with_score = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "    # Restore the original sentence order\n",
    "    ranked_sentences=list(dict(sorted((dict(map((lambda sentence: (\n",
    "        sentences.index(sentence),sentence) if sentence in sentences else (None,sentence)), dict(rnk_sent_with_score[:ranked_sentences_total]).values()))).items())).values())\n",
    "\n",
    "    return \"\\n\".join(ranked_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1 - Simplification -> summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(alignfile=None, cpu=False, criterion='cross_entropy', dataset_impl='cached', destdir='data/wiki-auto/100005/simplify_summary/binarized', fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=1, seed=1, source_lang='src', srcdict='simplification/preprocess/vocab_count.txt', target_lang='dst', task='translation', tbmf_wrapper=False, tensorboard_logdir='', testpref='data/wiki-auto/100005/simplify_summary/tokenized/test.tok', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, trainpref='data/wiki-auto/100005/simplify_summary/tokenized/train.tok', user_dir=None, validpref='data/wiki-auto/100005/simplify_summary/tokenized/valid.tok', workers=5)\n",
      "| [src] Dictionary: 30525 types\n",
      "| [src] data/wiki-auto/100005/simplify_summary/tokenized/train.tok.src: 63 sents, 2033 tokens, 0.0% replaced by <unk>\n",
      "| [src] Dictionary: 30525 types\n",
      "| [src] data/wiki-auto/100005/simplify_summary/tokenized/valid.tok.src: 63 sents, 2033 tokens, 0.0% replaced by <unk>\n",
      "| [src] Dictionary: 30525 types\n",
      "| [src] data/wiki-auto/100005/simplify_summary/tokenized/test.tok.src: 63 sents, 2033 tokens, 0.0% replaced by <unk>\n",
      "| [dst] Dictionary: 30525 types\n",
      "| [dst] data/wiki-auto/100005/simplify_summary/tokenized/train.tok.dst: 63 sents, 2033 tokens, 0.0% replaced by <unk>\n",
      "| [dst] Dictionary: 30525 types\n",
      "| [dst] data/wiki-auto/100005/simplify_summary/tokenized/valid.tok.dst: 63 sents, 2033 tokens, 0.0% replaced by <unk>\n",
      "| [dst] Dictionary: 30525 types\n",
      "| [dst] data/wiki-auto/100005/simplify_summary/tokenized/test.tok.dst: 63 sents, 2033 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to data/wiki-auto/100005/simplify_summary/binarized\n",
      "Traceback (most recent call last):\n",
      "  File \"simplification/generate.py\", line 192, in <module>\n",
      "    cli_main()\n",
      "  File \"simplification/generate.py\", line 188, in cli_main\n",
      "    main(args)\n",
      "  File \"simplification/generate.py\", line 49, in main\n",
      "    task=task,\n",
      "  File \"/mnt/nvme0n1p4/skool/master/UT/REDI/GAP/REDI-PROJECT/simplification/fairseq/checkpoint_utils.py\", line 156, in load_model_ensemble\n",
      "    ensemble, args, _task = load_model_ensemble_and_task(filenames, arg_overrides, task)\n",
      "  File \"/mnt/nvme0n1p4/skool/master/UT/REDI/GAP/REDI-PROJECT/simplification/fairseq/checkpoint_utils.py\", line 166, in load_model_ensemble_and_task\n",
      "    raise IOError('Model file not found: {}'.format(filename))\n",
      "OSError: Model file not found: checkpoints/checkpoint_wiki_auto.pt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'read_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d79690ddc69e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mgenerate_simplified_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinarized_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplification_output_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0msimplified_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimplification_output_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimplified_summary_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimplified_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4aa14a15dd03>\u001b[0m in \u001b[0;36mgenerate_summary\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Step 1 - Read text anc split it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# The number of sentences in the summarization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_file' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/wiki-auto'\n",
    "checkpoint_file = 'checkpoints/checkpoint_wiki_auto.pt'\n",
    "# count = 2\n",
    "\n",
    "# base data/wiki-auto\n",
    "for folder in os.listdir(data_dir):\n",
    "\t# raw data = data/wiki-auto/{id}\n",
    "\tbase_data_dir = os.path.join(data_dir, folder)\n",
    "\n",
    "\t# pipeline outputs = data/wiki-auto/{id}/simplify_summary\n",
    "\toutput_dir = os.path.join(base_data_dir, 'simplify_summary')\n",
    "\tos.makedirs(output_dir, exist_ok=True)\n",
    "\t\n",
    "\t# raw data = data/wiki-auto/{id}/simplify_summary/raw\n",
    "\t# tokenized data = data/wiki-auto/{id}/simplify_summary/tokenized\n",
    "\t# binarized data = data/wiki-auto/{id}/simplify_summary/binarized\n",
    "\traw_data_dir = os.path.join(output_dir, 'raw')\n",
    "\ttokenized_data_dir = os.path.join(output_dir, 'tokenized')\n",
    "\tbinarized_data_dir = os.path.join(output_dir, 'binarized')\n",
    "\n",
    "\t!rm -rf $raw_data_dir\n",
    "\t!rm -rf $tokenized_data_dir\n",
    "\t!rm -rf $binarized_data_dir\n",
    "\n",
    "\tos.makedirs(raw_data_dir, exist_ok=True)\n",
    "\tos.makedirs(tokenized_data_dir, exist_ok=True)\n",
    "\tos.makedirs(binarized_data_dir, exist_ok=True)\n",
    "\n",
    "\t# copy source.txt to simplify_summary/raw\n",
    "\t!cp $base_data_dir/source.txt $raw_data_dir/train.src\n",
    "\t!cp $base_data_dir/source.txt $raw_data_dir/train.dst\n",
    "\t!cp $base_data_dir/source.txt $raw_data_dir/test.src\n",
    "\t!cp $base_data_dir/source.txt $raw_data_dir/test.dst\n",
    "\t!cp $base_data_dir/source.txt $raw_data_dir/valid.src\n",
    "\t!cp $base_data_dir/source.txt $raw_data_dir/valid.dst\n",
    "\t\n",
    "\t# outputs in data/wiki-auto/{id}/simplify_summary/*.txt\n",
    "\tsimplification_output_file = os.path.join(output_dir, 'simplified.txt')\n",
    "\tsimplified_summary_file = os.path.join(output_dir, 'simplified_summary.txt')\n",
    "\n",
    "\tpreprocess_data(raw_data_dir, tokenized_data_dir, binarized_data_dir)\n",
    "\tgenerate_simplified_text(binarized_data_dir, simplification_output_file, checkpoint_file, 0, \"test\")\n",
    "\n",
    "\tsimplified_summary = generate_summary(simplification_output_file)\n",
    "\twith open(simplified_summary_file, 'w') as f:\n",
    "\t\tf.write(simplified_summary)\n",
    "\t\n",
    "\t# count -= 1\n",
    "\t# if count == 0:\n",
    "\t# \tbreak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2 - Summarization -> simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a94fb75f3c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msimplified_summary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'simplified_summary.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'source.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_output_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0msrc_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4aa14a15dd03>\u001b[0m in \u001b[0;36mgenerate_summary\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Step 1 - Read text anc split it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# The number of sentences in the summarization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_file' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/wiki-auto'\n",
    "checkpoint_file = 'checkpoints/checkpoint_wiki_auto.pt'\n",
    "# count = 2\n",
    "\n",
    "for folder in os.listdir(data_dir):\n",
    "\t# raw data = data/wiki-auto/{id}\n",
    "\tbase_data_dir = os.path.join(data_dir, folder)\n",
    "\n",
    "\t# pipeline outputs = data/wiki-auto/{id}/summary_simplify\n",
    "\toutput_dir = os.path.join(base_data_dir, 'summary_simplify')\n",
    "\tos.makedirs(output_dir, exist_ok=True)\n",
    "\t\n",
    "\t# raw data = data/wiki-auto/{id}/summary_simplify/raw\n",
    "\t# tokenized data = data/wiki-auto/{id}/summary_simplify/tokenized\n",
    "\t# binarized data = data/wiki-auto/{id}/summary_simplify/binarized\n",
    "\traw_data_dir = os.path.join(output_dir, 'raw')\t\n",
    "\ttokenized_data_dir = os.path.join(output_dir, 'tokenized')\n",
    "\tbinarized_data_dir = os.path.join(output_dir, 'binarized')\n",
    "\n",
    "\t!rm -rf $raw_data_dir\n",
    "\t!rm -rf $tokenized_data_dir\n",
    "\t!rm -rf $binarized_data_dir\n",
    "\t\n",
    "\tos.makedirs(raw_data_dir, exist_ok=True)\n",
    "\tos.makedirs(tokenized_data_dir, exist_ok=True)\n",
    "\tos.makedirs(binarized_data_dir, exist_ok=True)\n",
    "\t\n",
    "\t# outputs in data/wiki-auto/{id}/summary_simplify/*.txt\n",
    "\tsummary_output_file = os.path.join(output_dir, 'summary.txt')\n",
    "\tsimplified_summary_file = os.path.join(output_dir, 'simplified_summary.txt')\n",
    "\t\n",
    "\tsummary = generate_summary(os.path.join(base_data_dir, 'source.txt'))\n",
    "\twith open(summary_output_file, 'w') as src_f:\n",
    "\t\tsrc_f.write(summary)\n",
    "\t\n",
    "\t# copy summary.txt as raw/test.src and raw/test.dst\n",
    "\t!cp $summary_output_file $raw_data_dir/test.src\n",
    "\t!cp $summary_output_file $raw_data_dir/test.dst\n",
    "\t!cp $summary_output_file $raw_data_dir/train.src\n",
    "\t!cp $summary_output_file $raw_data_dir/train.dst\n",
    "\t!cp $summary_output_file $raw_data_dir/valid.src\n",
    "\t!cp $summary_output_file $raw_data_dir/valid.dst\n",
    "\n",
    "\tpreprocess_data(raw_data_dir, tokenized_data_dir, binarized_data_dir)\n",
    "\tgenerate_simplified_text(binarized_data_dir, simplified_summary_file, checkpoint_file, 0, \"test\")\n",
    "\t\n",
    "\t# count -= 1\n",
    "\t# if count == 0:\n",
    "\t# \tbreak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b2dca8fe6dbe9796b1dddfba88f9aac0a93c27145276b3838cc70c9e21c0725"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

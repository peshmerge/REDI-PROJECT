{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "# !pip install cdifflib\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define required methods\n",
    "\n",
    "Taken from `text_simplification.ipynb` and `simplified_summary_pipeline.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess test_data/raw test_data/tokenized test_data/binarized\n",
    "def preprocess_data(raw_data_dir, tokenized_data_dir, binarized_data_dir):\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/test.src --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/test.tok.src\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/test.dst --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/test.tok.dst \n",
    "\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/valid.src --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/valid.tok.src \n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/valid.dst --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/valid.tok.dst \n",
    "\n",
    "\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/train.src --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/train.tok.src \n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/train.dst --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/train.tok.dst \n",
    "\n",
    "\n",
    "    # Creates binarized fairseq dataset\n",
    "    !python simplification/preprocess.py --workers 5 --source-lang src --target-lang dst --trainpref $tokenized_data_dir/train.tok --validpref $tokenized_data_dir/valid.tok --testpref $tokenized_data_dir/test.tok --destdir  $binarized_data_dir --padding-factor 1 --joined-dictionary --srcdict simplification/preprocess/vocab_count.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simplified_text(binarized_data_dir, output_file, checkpoint_file, gpu_id=\"\", split=\"test\"):\n",
    "    !export CUDA_VISIBLE_DEVICES=$gpu_id\n",
    "    !python simplification/generate.py $binarized_data_dir --path $checkpoint_file --batch-size 32  --beam 1 --nbest 1 --user-dir simplification/my_model/ --print-alignment --gen-subset $split > $output_file'.aner'\n",
    "\n",
    "    !python simplification/postprocess/bpe.py  --out_anon $output_file'.aner' --denon $output_file --ignore_lines 5 --wp 1\n",
    "\n",
    "    !rm $output_file'.aner'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    return nltk.tokenize.sent_tokenize(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    "\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    "\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(file_name):\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    sentences =  read_file(file_name)\n",
    "\n",
    "    # The number of sentences in the summarization\n",
    "    ranked_sentences_total = (len(sentences)//2)+1\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    rnk_sent_with_score = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "    # Restore the original sentence order\n",
    "    ranked_sentences=list(dict(sorted((dict(map((lambda sentence: (\n",
    "        sentences.index(sentence),sentence) if sentence in sentences else (None,sentence)), dict(rnk_sent_with_score[:ranked_sentences_total]).values()))).items())).values())\n",
    "\n",
    "    return \"\\n\".join(ranked_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1 - Simplification -> summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/wiki-auto'\n",
    "checkpoint_file = 'checkpoints/checkpoint_wiki_auto.pt'\n",
    "# count = 2\n",
    "\n",
    "# base data/wiki-auto\n",
    "for folder in os.listdir(data_dir):\n",
    "\t# raw data = data/wiki-auto/{id}\n",
    "\tbase_data_dir = os.path.join(data_dir, folder)\n",
    "\n",
    "\t# pipline outputs = data/wiki-auto/{id}/simplify_summary\n",
    "\toutput_dir = os.path.join(base_data_dir, 'simplify_summary')\n",
    "\tos.makedirs(output_dir, exist_ok=True)\n",
    "\t\n",
    "\t# raw data = data/wiki-auto/{id}/simplify_summary/raw\n",
    "\t# tokenized data = data/wiki-auto/{id}/simplify_summary/tokenized\n",
    "\t# binarized data = data/wiki-auto/{id}/simplify_summary/binarized\n",
    "\traw_data_dir = os.path.join(output_dir, 'raw')\n",
    "\ttokenized_data_dir = os.path.join(output_dir, 'tokenized')\n",
    "\tbinarized_data_dir = os.path.join(output_dir, 'binarized')\n",
    "\n",
    "\t!rm -rf $raw_data_dir\n",
    "\t!rm -rf $tokenized_data_dir\n",
    "\t!rm -rf $binarized_data_dir\n",
    "\n",
    "\tos.makedirs(raw_data_dir, exist_ok=True)\n",
    "\tos.makedirs(tokenized_data_dir, exist_ok=True)\n",
    "\tos.makedirs(binarized_data_dir, exist_ok=True)\n",
    "\n",
    "\t# copy source.txt to simplify_summary/raw\n",
    "\t!cp $base_data_dir/source.txt $raw_data_dir/train.src\n",
    "\t!cp $base_data_dir/source.txt $raw_data_dir/train.dst\n",
    "\t!cp $base_data_dir/source.txt $raw_data_dir/test.src\n",
    "\t!cp $base_data_dir/source.txt $raw_data_dir/test.dst\n",
    "\t!cp $base_data_dir/source.txt $raw_data_dir/valid.src\n",
    "\t!cp $base_data_dir/source.txt $raw_data_dir/valid.dst\n",
    "\t\n",
    "\t# outputs in data/wiki-auto/{id}/simplify_summary/*.txt\n",
    "\tsimplification_output_file = os.path.join(output_dir, 'simplified.txt')\n",
    "\tsimplified_summary_file = os.path.join(output_dir, 'simplified_summary.txt')\n",
    "\n",
    "\tpreprocess_data(raw_data_dir, tokenized_data_dir, binarized_data_dir)\n",
    "\tgenerate_simplified_text(binarized_data_dir, simplification_output_file, checkpoint_file, 0, \"test\")\n",
    "\n",
    "\tsimplified_summary = generate_summary(simplification_output_file)\n",
    "\twith open(simplified_summary_file, 'w') as f:\n",
    "\t\tf.write(simplified_summary)\n",
    "\t\n",
    "\t# count -= 1\n",
    "\t# if count == 0:\n",
    "\t# \tbreak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2 - Summarization -> simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/wiki-auto'\n",
    "checkpoint_file = 'checkpoints/checkpoint_wiki_auto.pt'\n",
    "# count = 2\n",
    "\n",
    "for folder in os.listdir(data_dir):\n",
    "\t# raw data = data/wiki-auto/{id}\n",
    "\tbase_data_dir = os.path.join(data_dir, folder)\n",
    "\n",
    "\t# pipline outputs = data/wiki-auto/{id}/summary_simplify\n",
    "\toutput_dir = os.path.join(base_data_dir, 'summary_simplify')\n",
    "\tos.makedirs(output_dir, exist_ok=True)\n",
    "\t\n",
    "\t# raw data = data/wiki-auto/{id}/summary_simplify/raw\n",
    "\t# tokenized data = data/wiki-auto/{id}/summary_simplify/tokenized\n",
    "\t# binarized data = data/wiki-auto/{id}/summary_simplify/binarized\n",
    "\traw_data_dir = os.path.join(output_dir, 'raw')\t\n",
    "\ttokenized_data_dir = os.path.join(output_dir, 'tokenized')\n",
    "\tbinarized_data_dir = os.path.join(output_dir, 'binarized')\n",
    "\n",
    "\t!rm -rf $raw_data_dir\n",
    "\t!rm -rf $tokenized_data_dir\n",
    "\t!rm -rf $binarized_data_dir\n",
    "\t\n",
    "\tos.makedirs(raw_data_dir, exist_ok=True)\n",
    "\tos.makedirs(tokenized_data_dir, exist_ok=True)\n",
    "\tos.makedirs(binarized_data_dir, exist_ok=True)\n",
    "\t\n",
    "\t# outputs in data/wiki-auto/{id}/summary_simplify/*.txt\n",
    "\tsummary_output_file = os.path.join(output_dir, 'summary.txt')\n",
    "\tsimplified_summary_file = os.path.join(output_dir, 'simplified_summary.txt')\n",
    "\t\n",
    "\tsummary = generate_summary(os.path.join(base_data_dir, 'source.txt'))\n",
    "\twith open(summary_output_file, 'w') as src_f:\n",
    "\t\tsrc_f.write(summary)\n",
    "\t\n",
    "\t# copy summaty.txt as raw/test.src and raw/test.dst\n",
    "\t!cp $summary_output_file $raw_data_dir/test.src\n",
    "\t!cp $summary_output_file $raw_data_dir/test.dst\n",
    "\t!cp $summary_output_file $raw_data_dir/train.src\n",
    "\t!cp $summary_output_file $raw_data_dir/train.dst\n",
    "\t!cp $summary_output_file $raw_data_dir/valid.src\n",
    "\t!cp $summary_output_file $raw_data_dir/valid.dst\n",
    "\n",
    "\tpreprocess_data(raw_data_dir, tokenized_data_dir, binarized_data_dir)\n",
    "\tgenerate_simplified_text(binarized_data_dir, simplified_summary_file, checkpoint_file, 0, \"test\")\n",
    "\t\n",
    "\t# count -= 1\n",
    "\t# if count == 0:\n",
    "\t# \tbreak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b05898d6406f5fb4cb652f7e2279c0b443d9b6858086f059923d2604af447e74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

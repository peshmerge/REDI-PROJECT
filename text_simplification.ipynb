{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text simplification \n",
    "using code provided by https://github.com/chaojiang06/wiki-auto\n",
    "Paper link: https://arxiv.org/abs/2005.02324"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need tensorflow 2 to be installed to run the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess test_data/raw test_data/tokenized test_data/binarized\n",
    "def preprocess_data(raw_data_dir,tokenized_data_dir,binarized_data_dir):\n",
    "    !rm -r $binarized_data_dir/*\n",
    "    !rm -r $tokenized_data_dir/*\n",
    "\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/test.src --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/test.tok.src\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/test.dst --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/test.tok.dst\n",
    "\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/valid.src --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/valid.tok.src\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/valid.dst --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/valid.tok.dst\n",
    "\n",
    "\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/train.src --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/train.tok.src\n",
    "    !python simplification/preprocess/anonymize_wordpiece.py --input $raw_data_dir/train.dst --vocab simplification/preprocess/vocab.txt --output  $tokenized_data_dir/train.tok.dst\n",
    "\n",
    "\n",
    "    # Creates binarized fairseq dataset\n",
    "    !python simplification/preprocess.py --workers 5 --source-lang src --target-lang dst --trainpref $tokenized_data_dir/train.tok --validpref $tokenized_data_dir/valid.tok --testpref $tokenized_data_dir/test.tok --destdir  $binarized_data_dir --padding-factor 1 --joined-dictionary --srcdict simplification/preprocess/vocab_count.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(alignfile=None, cpu=False, criterion='cross_entropy', dataset_impl='cached', destdir='simplification/test_data/binarized', fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=1, seed=1, source_lang='src', srcdict='simplification/preprocess/vocab_count.txt', target_lang='dst', task='translation', tbmf_wrapper=False, tensorboard_logdir='', testpref='simplification/test_data/tokenized/test.tok', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, trainpref='simplification/test_data/tokenized/train.tok', user_dir=None, validpref='simplification/test_data/tokenized/valid.tok', workers=5)\n",
      "| [src] Dictionary: 30525 types\n",
      "| [src] simplification/test_data/tokenized/train.tok.src: 50 sents, 2205 tokens, 0.0% replaced by <unk>\n",
      "| [src] Dictionary: 30525 types\n",
      "| [src] simplification/test_data/tokenized/valid.tok.src: 10 sents, 382 tokens, 0.0% replaced by <unk>\n",
      "| [src] Dictionary: 30525 types\n",
      "| [src] simplification/test_data/tokenized/test.tok.src: 10 sents, 460 tokens, 0.0% replaced by <unk>\n",
      "| [dst] Dictionary: 30525 types\n",
      "| [dst] simplification/test_data/tokenized/train.tok.dst: 50 sents, 1872 tokens, 0.0% replaced by <unk>\n",
      "| [dst] Dictionary: 30525 types\n",
      "| [dst] simplification/test_data/tokenized/valid.tok.dst: 10 sents, 355 tokens, 0.0% replaced by <unk>\n",
      "| [dst] Dictionary: 30525 types\n",
      "| [dst] simplification/test_data/tokenized/test.tok.dst: 10 sents, 287 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to simplification/test_data/binarized\n"
     ]
    }
   ],
   "source": [
    "preprocess_data('simplification/test_data/raw','simplification/test_data/tokenized','simplification/test_data/binarized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text simplification generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating using checkpoint {'simplification/checkpoints/checkpoint_wiki_auto.pt'}\n",
      "\n",
      "simplification/0000.aner\n"
     ]
    }
   ],
   "source": [
    "def generate_simplified_text(binarized_data_dir,output_file,checkpoint_file, gpu_id=\"\", split=\"test\"):\n",
    "    print(\"Generating using checkpoint\" , {checkpoint_file})\n",
    "    print()\n",
    "    !echo $output_file'.aner'\n",
    "\n",
    "    !export CUDA_VISIBLE_DEVICES=$gpu_id \n",
    "    !python simplification/generate.py $binarized_data_dir --path $checkpoint_file --batch-size 32  --beam 1 --nbest 1 --user-dir simplification/my_model/ --print-alignment --gen-subset $split > $output_file'.aner'\n",
    "\n",
    "    !python simplification/postprocess/bpe.py  --out_anon $output_file'.aner' --denon $output_file --ignore_lines 5 --wp 1\n",
    "\n",
    "    !rm $output_file'.aner'\n",
    "generate_simplified_text('simplification/test_data/binarized','simplification/0000','simplification/checkpoints/checkpoint_wiki_auto.pt',0,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating using checkpoint {'simplification/checkpoints/checkpoint_wiki_auto.pt'}\n",
      "\n",
      "usage: generate.py [-h] [--no-progress-bar] [--log-interval N]\n",
      "                   [--log-format {json,none,simple,tqdm}]\n",
      "                   [--tensorboard-logdir DIR] [--tbmf-wrapper] [--seed N]\n",
      "                   [--cpu] [--fp16] [--memory-efficient-fp16]\n",
      "                   [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                   [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                   [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                   [--min-loss-scale D]\n",
      "                   [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\n",
      "                   [--user-dir USER_DIR]\n",
      "                   [--criterion {adaptive_loss,binary_cross_entropy,composite_loss,cross_entropy,label_smoothed_cross_entropy,masked_lm_loss}]\n",
      "                   [--optimizer {adadelta,adafactor,adagrad,adam,lamb,nag,sgd}]\n",
      "                   [--lr-scheduler {cosine,fixed,inverse_sqrt,polynomial_decay,reduce_lr_on_plateau,triangular}]\n",
      "                   [--task TASK] [--num-workers N]\n",
      "                   [--skip-invalid-size-inputs-valid-test] [--max-tokens N]\n",
      "                   [--max-sentences N] [--required-batch-size-multiple N]\n",
      "                   [--dataset-impl FORMAT] [--gen-subset SPLIT]\n",
      "                   [--num-shards N] [--shard-id ID] [--path FILE]\n",
      "                   [--remove-bpe [REMOVE_BPE]] [--quiet]\n",
      "                   [--model-overrides DICT] [--results-path RESDIR] [--beam N]\n",
      "                   [--nbest N] [--max-len-a N] [--max-len-b N] [--min-len-a N]\n",
      "                   [--min-len-b N] [--min-len N] [--match-source-len]\n",
      "                   [--no-early-stop] [--unnormalized] [--no-beamable-mm]\n",
      "                   [--lenpen LENPEN] [--unkpen UNKPEN]\n",
      "                   [--replace-unk [REPLACE_UNK]] [--sacrebleu]\n",
      "                   [--score-reference] [--prefix-size PS]\n",
      "                   [--no-repeat-ngram-size N] [--sampling]\n",
      "                   [--sampling-topk PS] [--sampling-topp PS] [--temperature N]\n",
      "                   [--diverse-beam-groups N] [--diverse-beam-strength N]\n",
      "                   [--print-alignment]\n",
      "generate.py: error: argument --path: expected one argument\n",
      "usage: bpe.py [-h] [--out_anon OUT_ANON] [--denon DENON] [--wp WP]\n",
      "              [--ignore_lines IGNORE_LINES] [--interactive INTERACTIVE]\n",
      "bpe.py: error: argument --denon: expected one argument\n",
      ".aner\n"
     ]
    }
   ],
   "source": [
    "generate_simplified_text('simplification/test_data/binarized','simplification/0000.txt','simplification/checkpoints/checkpoint_wiki_auto.pt',0,\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b2dca8fe6dbe9796b1dddfba88f9aac0a93c27145276b3838cc70c9e21c0725"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

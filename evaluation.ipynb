{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "import json\n",
    "import pandas as pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path,'r', encoding=\"utf8\") as file:\n",
    "        return file.readlines()\n",
    "\n",
    "def read_file_as_text(file_path):\n",
    "    with open(file_path,'r', encoding=\"utf8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_reference_file_for_bleu_metric(path):\n",
    "    with open(path,'r', encoding=\"utf8\") as file:\n",
    "        # Returns a list of lists (each sentence is a list itself, with one item)\n",
    "        return [list(map(str, line.rstrip(\",\\r\\n\").split(\",\"))) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = 'simplify_summary'\n",
    "p2 = 'summary_simplify'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_for_dataset(metric, pipeline=p1, data_dir='data/'):\n",
    "    '''\n",
    "    General method that takes in the metric method and returns the metric scores\n",
    "    '''\n",
    "    evaluation_dataset = dict()\n",
    "    metric_name = metric.__name__\n",
    "    wiki_base_dir = os.path.join(data_dir, 'wiki-auto')\n",
    "\n",
    "    processed_articles = read_file_as_text('data/processed_articles.txt').split('\\n')\n",
    "\n",
    "    # base data/wiki-auto\n",
    "    for folder in processed_articles:\n",
    "        base_data_dir = os.path.join(wiki_base_dir, folder)\n",
    "        ground_truth_file_path = os.path.join(base_data_dir, 'destination.txt')\n",
    "        pipeline_dir = os.path.join(base_data_dir, pipeline)\n",
    "        simplified_summary_file_path = os.path.join(pipeline_dir, 'simplified_summary.txt')\n",
    "        if os.path.exists(simplified_summary_file_path):\n",
    "            evaluation_dataset[folder] = metric(simplified_summary_file_path, ground_truth_file_path)\n",
    "            print(f'Done generating {metric_name} scores for {folder}\\r', end='', flush=True)\n",
    "    with open(f'{data_dir}{pipeline}_{metric_name}', 'w') as outputfile:\n",
    "        json.dump(evaluation_dataset, outputfile)\n",
    "    print('\\nDone')\n",
    "    return evaluation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical similarity based scores - ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE - ROUGE-1, ROUGE-2, ROUGE-L and Google ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge\n",
    "!pip install pandas\n",
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_scores(hyp_file_path, ref_file_path):\n",
    "    '''\n",
    "    Calculate ROUGE-1, ROUGE-2 and ROUGE-l scores for hypothesis and reference files\n",
    "    '''\n",
    "    rouge = Rouge()\n",
    "    hypotheses = ''\n",
    "    references = ''\n",
    "    hypotheses = read_file_as_text(hyp_file_path)\n",
    "    references = read_file_as_text(ref_file_path)\n",
    "    return rouge.get_scores(hypotheses, references, avg=True)\n",
    "\n",
    "# pprint(calculate_rouge_score('data/wiki-auto/25/simplify_summary/simplified_summary.txt','data/wiki-auto/25/destination.txt'))\n",
    "# calculate_rouge_score('data/wiki-auto/25/summary_simplify/simplified_summary.txt','data/wiki-auto/25/destination.txt')\n",
    "\n",
    "\n",
    "def convert_google_rouge_format_to_usable_format(scores):\n",
    "    google_rouge_stats = dict(\n",
    "    {\n",
    "        'rouge-1': {\n",
    "            \"r\": 0,\n",
    "            \"p\": 0,\n",
    "            \"f\": 0,\n",
    "        },\n",
    "        'rouge-2': {\n",
    "            \"r\": 0,\n",
    "            \"p\": 0,\n",
    "            \"f\": 0,\n",
    "        },\n",
    "        'rouge-l': {\n",
    "            \"r\": 0,\n",
    "            \"p\": 0,\n",
    "            \"f\": 0,\n",
    "        },\n",
    "        'rouge-lsum': {\n",
    "            \"r\": 0,\n",
    "            \"p\": 0,\n",
    "            \"f\": 0,\n",
    "        }\n",
    "        \n",
    "    })\n",
    "    for key in scores:\n",
    "        if key == 'rouge1':\n",
    "            google_rouge_stats['rouge-1'] = {\"p\":scores[key].precision,\"r\":scores[key].recall,\"f\":scores[key].fmeasure}\n",
    "        if key == 'rouge2':\n",
    "            google_rouge_stats['rouge-2'] = {\"p\":scores[key].precision,\"r\":scores[key].recall,\"f\":scores[key].fmeasure}\n",
    "        if key == 'rougeL':\n",
    "            google_rouge_stats['rouge-l'] = {\"p\":scores[key].precision,\"r\":scores[key].recall,\"f\":scores[key].fmeasure}\n",
    "        if key == 'rougeLsum':\n",
    "            google_rouge_stats['rouge-lsum'] = {\"p\":scores[key].precision,\"r\":scores[key].recall,\"f\":scores[key].fmeasure}\n",
    "    return google_rouge_stats\n",
    "\n",
    "def google_rouge_scores(hyp_file_path, ref_file_path):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL','rougeLsum'], use_stemmer=True)\n",
    "    hypotheses = read_file_as_text(hyp_file_path)\n",
    "    references = read_file_as_text(ref_file_path)\n",
    "    scores = scorer.score(references,hypotheses)\n",
    "    # pprint(type(scores))\n",
    "    # return scores\n",
    "    return convert_google_rouge_format_to_usable_format(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.5666280417149478,\n",
      "             'p': 0.6680327868852459,\n",
      "             'r': 0.4919517102615694},\n",
      " 'rouge-2': {'f': 0.1654592871631411,\n",
      "             'p': 0.1950802869832593,\n",
      "             'r': 0.14364779874213837},\n",
      " 'rouge-l': {'f': 0.14629200463499423,\n",
      "             'p': 0.17247267759562843,\n",
      "             'r': 0.12701207243460766},\n",
      " 'rouge-lsum': {'f': 0.5559096176129779,\n",
      "                'p': 0.655396174863388,\n",
      "                'r': 0.48264587525150904}}\n"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "pprint(google_rouge_scores('data/wiki-auto/25/simplify_summary/simplified_summary.txt','data/wiki-auto/25/destination.txt'))\n",
    "# pprint(calculate_google_rouge_score('data/wiki-auto/25/summary_simplify/simplified_summary.txt','data/wiki-auto/25/destination.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 5s, sys: 540 ms, total: 10min 6s\n",
      "Wall time: 10min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate ROUGE score for the whole dataset for the first pipeline (Simplify & Summarize)\n",
    "rouge_p1 = calculate_metrics_for_dataset(rouge_scores, pipeline=p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 55s, sys: 724 ms, total: 9min 56s\n",
      "Wall time: 9min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate ROUGE score for the whole dataset for the second pipeline (Summarize & Simplify)\n",
    "rouge_p2 = calculate_metrics_for_dataset(rouge_scores, pipeline=p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 5s, sys: 3.72 s, total: 11min 9s\n",
      "Wall time: 11min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate Google ROUGE score for the whole dataset for the first pipeline (Simplify & Summarize)\n",
    "google_rouge_p1 = calculate_metrics_for_dataset(google_rouge_scores, pipeline=p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 18s, sys: 2.73 s, total: 10min 21s\n",
      "Wall time: 10min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate Google ROUGE score for the whole dataset for the second pipeline (Summarize & Simplify)\n",
    "google_rouge_p2 = calculate_metrics_for_dataset(google_rouge_scores, pipeline=p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate average values for whole dataset\n",
    "\n",
    "Next cell loads data from files if the actual evaluation is not performed and the scores from the file is to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('data/simplify_summary_rouge_scores', 'r', encoding=\"utf8\") as f:\n",
    "    rouge_p1 = json.load(f)\n",
    "\n",
    "with open('data/summary_simplify_rouge_scores', 'r', encoding=\"utf8\") as f:\n",
    "    rouge_p2 = json.load(f)\n",
    "\n",
    "with open('data/summary_simplify_google_rouge_scores', 'r', encoding=\"utf8\") as f:\n",
    "    google_rouge_p1 = json.load(f)\n",
    "\n",
    "with open('data/summary_simplify_google_rouge_scores', 'r', encoding=\"utf8\") as f:\n",
    "    google_rouge_p2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_rouge_stats(rouge_dataframe, isGoogle=False):\n",
    "    '''\n",
    "    Calculate the average ROUGE scores r,p,f for the whole given dataset \n",
    "    '''\n",
    "    stats = {}\n",
    "    cols=['rouge-1', 'rouge-2', 'rouge-l']\n",
    "    if isGoogle:\n",
    "        cols.append('rouge-lsum')\n",
    "    dataset_size = len(rouge_dataframe)\n",
    "    for col in cols:\n",
    "        stats[col] = {}\n",
    "        stats[col]['r'] = round(sum(x[col]['r'] for x in rouge_dataframe.values())/dataset_size, 3)\n",
    "        stats[col]['p'] = round(sum(x[col]['p'] for x in rouge_dataframe.values())/dataset_size, 3)\n",
    "        stats[col]['f'] = round(sum(x[col]['f'] for x in rouge_dataframe.values())/dataset_size, 3)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROUGE-1,ROUGE-2 and ROUGE-l score for P1 - Simplify & Summary : \n",
      "{'rouge-1': {'f': 0.309, 'p': 0.344, 'r': 0.297},\n",
      " 'rouge-2': {'f': 0.129, 'p': 0.145, 'r': 0.125},\n",
      " 'rouge-l': {'f': 0.292, 'p': 0.326, 'r': 0.28}}\n",
      "The ROUGE-1,ROUGE-2 and ROUGE-l score for P2 - Summary & Simplify : \n",
      "{'rouge-1': {'f': 0.309, 'p': 0.352, 'r': 0.291},\n",
      " 'rouge-2': {'f': 0.13, 'p': 0.151, 'r': 0.123},\n",
      " 'rouge-l': {'f': 0.293, 'p': 0.333, 'r': 0.275}}\n"
     ]
    }
   ],
   "source": [
    "print(\"The ROUGE-1,ROUGE-2 and ROUGE-l score for P1 - Simplify & Summary : \")\n",
    "pprint(calculate_avg_rouge_stats(rouge_p1))\n",
    "\n",
    "print(\"The ROUGE-1,ROUGE-2 and ROUGE-l score for P2 - Summary & Simplify : \")\n",
    "pprint(calculate_avg_rouge_stats(rouge_p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Google ROUGE-1, ROUGE-2, ROUGE-l and ROUGE-Lsum score for P1 - Simplify & Summary : \n",
      "{'rouge-1': {'f': 0.509, 'p': 0.594, 'r': 0.478},\n",
      " 'rouge-2': {'f': 0.228, 'p': 0.274, 'r': 0.21},\n",
      " 'rouge-l': {'f': 0.272, 'p': 0.326, 'r': 0.251},\n",
      " 'rouge-lsum': {'f': 0.494, 'p': 0.576, 'r': 0.464}}\n",
      "The Google ROUGE-1, ROUGE-2, ROUGE-l and ROUGE-Lsum score for P2 - Summary & Simplify : \n",
      "{'rouge-1': {'f': 0.509, 'p': 0.594, 'r': 0.478},\n",
      " 'rouge-2': {'f': 0.228, 'p': 0.274, 'r': 0.21},\n",
      " 'rouge-l': {'f': 0.272, 'p': 0.326, 'r': 0.251},\n",
      " 'rouge-lsum': {'f': 0.494, 'p': 0.576, 'r': 0.464}}\n"
     ]
    }
   ],
   "source": [
    "print(\"The Google ROUGE-1, ROUGE-2, ROUGE-l and ROUGE-Lsum score for P1 - Simplify & Summary : \")\n",
    "pprint(calculate_avg_rouge_stats(google_rouge_p1, isGoogle=True))\n",
    "\n",
    "print(\"The Google ROUGE-1, ROUGE-2, ROUGE-l and ROUGE-Lsum score for P2 - Summary & Simplify : \")\n",
    "pprint(calculate_avg_rouge_stats(google_rouge_p2, isGoogle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_rouge_scores(rf, isGoogle=False):\n",
    "\t'''\n",
    "\tGet minimum and maximum rouge scores from rf as a dict\n",
    "\t'''\n",
    "\tcols=['rouge-1', 'rouge-2', 'rouge-l']\n",
    "\tif isGoogle:\n",
    "\t\tcols.append('rouge-lsum')\n",
    "\tstats = {}\n",
    "\tfor metric in cols:\n",
    "\t\tstats[metric] = {}\n",
    "\t\tstats[metric]['max'] = {}\n",
    "\t\tstats[metric]['max'] = round(max(x[metric]['r'] for x in rf.values()), 5)\n",
    "\t\tstats[metric]['max'] = round(max(x[metric]['p'] for x in rf.values()), 5)\n",
    "\t\tstats[metric]['max'] = round(max(x[metric]['f'] for x in rf.values()), 5)\n",
    "\t\tstats[metric]['min'] = {}\n",
    "\t\tstats[metric]['min'] = round(min(x[metric]['r'] for x in rf.values()), 5)\n",
    "\t\tstats[metric]['min'] = round(min(x[metric]['p'] for x in rf.values()), 5)\n",
    "\t\tstats[metric]['min'] = round(min(x[metric]['f'] for x in rf.values()), 5)\n",
    "\treturn stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'google_p1': {'rouge-1': {'max': 0.81702, 'min': 0.09167},\n",
      "               'rouge-2': {'max': 0.79828, 'min': 0.02473},\n",
      "               'rouge-l': {'max': 0.81702, 'min': 0.055},\n",
      "               'rouge-lsum': {'max': 0.81702, 'min': 0.08843}},\n",
      " 'google_p2': {'rouge-1': {'max': 0.81702, 'min': 0.09167},\n",
      "               'rouge-2': {'max': 0.79828, 'min': 0.02473},\n",
      "               'rouge-l': {'max': 0.81702, 'min': 0.055},\n",
      "               'rouge-lsum': {'max': 0.81702, 'min': 0.08843}},\n",
      " 'p1': {'rouge-1': {'max': 0.64474, 'min': 0.07303},\n",
      "        'rouge-2': {'max': 0.45026, 'min': 0.00685},\n",
      "        'rouge-l': {'max': 0.64474, 'min': 0.0618}},\n",
      " 'p2': {'rouge-1': {'max': 0.60606, 'min': 0.07386},\n",
      "        'rouge-2': {'max': 0.41833, 'min': 0.00698},\n",
      "        'rouge-l': {'max': 0.60606, 'min': 0.0625}}}\n"
     ]
    }
   ],
   "source": [
    "all_rouge_stats = {}\n",
    "all_rouge_stats['p1'] = get_min_max_rouge_scores(rouge_p1)\n",
    "all_rouge_stats['p2'] = get_min_max_rouge_scores(rouge_p2)\n",
    "all_rouge_stats['google_p1'] = get_min_max_rouge_scores(google_rouge_p1, isGoogle=True)\n",
    "all_rouge_stats['google_p2'] = get_min_max_rouge_scores(google_rouge_p2, isGoogle=True)\n",
    "pprint(all_rouge_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyterlab pandas datasets matplotlib plotly scikit-learn tqdm ipywidgets \n",
    "!pip install numpy spacy textdistance fasttext \n",
    "!pip install tensorflow tensorflow_hub sentence-transformers openai\n",
    "!conda install pyemd gensim\n",
    "\n",
    "# Download the Spacy Model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code based on [this](https://towardsdatascience.com/semantic-textual-similarity-83b3ca4a840e) turorial:\n",
    "\n",
    "### Word Movers Distance (WMD)\n",
    "\n",
    "> 💡 Word embeddings are models that encode words into numeric vectors such that similar words have vectors that are near each other in vector space.\n",
    "\n",
    "There are several ways to generate word embeddings, the most prominent being Word2Vec, GloVe, and FastText.\n",
    "\n",
    "Since we need to compare the similarity between texts that contain multiple words, the simplest way to go from individual word embeddings into a single sentence embedding is to calculate the element-wise average of all the word embeddings in that text. However, there is an even better approach to computing the similarity between texts directly from the word embeddings called Word Movers Distance (WMD).\n",
    "\n",
    "[WMD](http://proceedings.mlr.press/v37/kusnerb15.pdf) is based on the concept of [Earth Movers Distance](https://www.cs.jhu.edu/~misha/Papers/Rubner98.pdf) and is the minimum distance that the word embeddings from one document need to “travel” to reach the word embeddings of the document we are comparing it to. Since each document includes multiple words, the WMD calculation needs to calculate the distances from each word to every other word. It also weights the “travel” by the term frequencies of each word. Thankfully, the [gensim](https://github.com/RaRe-Technologies/gensim) library implements this complex computation efficiently using the [Fast WMD algorithm](https://www.cs.huji.ac.il/w~werman/Papers/ICCV2009.pdf). We can easily use it with just a single line of code!\n",
    "\n",
    "Though we can use any word embedding model with WMD, I decide to use the [FastText model](https://arxiv.org/pdf/1607.04606.pdf) pre-trained on Wikipedia primarily because FastText uses sub-word information and will never run into Out Of Vocabulary issues that Word2Vec or GloVe might encounter. Take note to preprocess the texts to remove stopwords, lower case, and lemmatize them to ensure that the WMD calculation only uses informative words. Finally, since the WMD is a distance metric while we are looking for a similarity metric, we multiply the WMD value by -1 (Negative WMD) so that more similar texts have numerically larger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(sentence):\n",
    "    sentence = [token.lemma_.lower()\n",
    "                for token in nlp(sentence) \n",
    "                if token.is_alpha and not token.is_stop]   \n",
    "    return sentence\n",
    "\n",
    "def cos_sim(sentence1_emb, sentence2_emb):\n",
    "    cos_sim = cosine_similarity(sentence1_emb, sentence2_emb)\n",
    "    return np.diag(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! Run the next cell only once to load the model !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load the pre-trained model\n",
    "model = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmd_scores(file1, file2):\n",
    "    # load text from file as a gle sentence\n",
    "    text1 = ' '.join(read_file_as_text(file1).split('\\n'))\n",
    "    text2 = ' '.join(read_file_as_text(file2).split('\\n'))\n",
    "    # Text Processing\n",
    "    processed_text1 = text_processing(text1)\n",
    "    processed_text2 = text_processing(text2)\n",
    "    \n",
    "    # Negative Word Movers Distance\n",
    "    return -model.wmdistance(processed_text1, processed_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating wmd_scores scores for 99569\n",
      "Done\n",
      "Wall time: 1h 27min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate WMD similarity for the whole dataset for the first pipeline (Simplify & Summarize)\n",
    "wmd_p1 = calculate_metrics_for_dataset(wmd_scores, pipeline=p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating wmd_scores scores for 99569\n",
      "Done\n",
      "Wall time: 1h 15min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate WMD similarity for the whole dataset for the second pipeline (Summarize & Simplify)\n",
    "wmd_p2 = calculate_metrics_for_dataset(wmd_scores, pipeline=p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.03 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('data/simplify_summary_wmd_scores', 'r', encoding=\"utf8\") as p1_f:\n",
    "    wmd_p1 = json.load(p1_f)\n",
    "\n",
    "with open('data/summary_simplify_wmd_scores', 'r', encoding=\"utf8\") as p2_f:\n",
    "    wmd_p2 = json.load(p2_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_wmd_stats(wmd_scores):\n",
    "\t'''\n",
    "\tGet basic WMD stats - average, min and max scores\n",
    "\t'''\n",
    "\tstats = {}\n",
    "\tstats['average'] = -round(sum(wmd_scores.values())/len(wmd_scores), 3)\n",
    "\tstats['max_score'] = -round(max(wmd_scores.values()), 3)\n",
    "\tstats['min_score'] = -round(min(wmd_scores.values()), 3)\n",
    "\treturn stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Word Mover Distance similarity scores for P1 - Simplify & Summary : \n",
      "{'average': 0.589, 'max_score': 0.27, 'min_score': 1.028}\n",
      "The Word Mover Distance similarity scores for P1 - Summary & Simplify : \n",
      "{'average': 0.592, 'max_score': 0.242, 'min_score': 0.993}\n"
     ]
    }
   ],
   "source": [
    "print(\"The Word Mover Distance similarity scores for P1 - Simplify & Summary : \")\n",
    "pprint(get_basic_wmd_stats(wmd_p1))\n",
    "\n",
    "print(\"The Word Mover Distance similarity scores for P1 - Summary & Simplify : \")\n",
    "pprint(get_basic_wmd_stats(wmd_p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge wmd scores for p1 and p2\n",
    "wmd = {}\n",
    "for key in wmd_p1:\n",
    "\twmd[key] = {}\n",
    "\twmd[key]['p1'] = round(wmd_p1[key], 3)\n",
    "\twmd[key]['p2'] = round(wmd_p2[key], 3)\n",
    "\twmd[key]['diff'] = round(abs(wmd_p2[key] - wmd_p1[key]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WMD difference in score:  0.026702\n",
      "Max WMD difference in score:  0.247\n",
      "Min WMD difference in score:  0.0\n"
     ]
    }
   ],
   "source": [
    "l = 500\n",
    "\n",
    "print('Average WMD difference in score: ', round(sum(wmd[art]['diff'] for art in wmd.keys())/l, 6))\n",
    "print('Max WMD difference in score: ', round(max(wmd[art]['diff'] for art in wmd.keys()), 6))\n",
    "print('Min WMD difference in score: ', round(min(wmd[art]['diff'] for art in wmd.keys()), 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b5c60c725058d497af2559aff8f511a78a8d7ce6eb23d02d51aff3d8215ce57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
